# ü©∫ Predicci√≥n de Glucosa con Machine Learning

## üìã Descripci√≥n General

Este proyecto desarrolla y compara **7 modelos de Machine Learning** diferentes para predecir niveles de glucosa en sangre bas√°ndose en caracter√≠sticas cl√≠nicas y antropom√©tricas del paciente. Todos los modelos siguen la misma metodolog√≠a y pipeline, variando solo el algoritmo de entrenamiento.

Los modelos entrenados son:
1. Regresi√≥n Lineal
2. Ridge Regression
3. Lasso Regression
4. Random Forest
5. **Gradient Boosting** (usado como ejemplo en esta documentaci√≥n)
6. Support Vector Machine (SVM)
7. Red Neuronal (MLP)

---

## üéØ Objetivo

- Desarrollar m√∫ltiples modelos de predicci√≥n de glucosa
- Comparar desempe√±o entre diferentes algoritmos
- Identificar el modelo √≥ptimo para m√°xima precisi√≥n
- Proporcionar una estructura reutilizable para cada modelo
- Clasificar autom√°ticamente resultados seg√∫n criterios cl√≠nicos

---

## üì¶ Dependencias Requeridas

### Instalaci√≥n

```bash
pip install pandas numpy scikit-learn joblib
```

### Versiones Recomendadas

```bash
pip install pandas>=1.3.0 numpy>=1.21.0 scikit-learn>=1.0.0 joblib>=1.0.0
```

---

## üîß Librer√≠as Utilizadas

### **Pandas**
```python
import pandas as pd
```

| Funci√≥n | Descripci√≥n |
|---------|-------------|
| `pd.read_csv()` | Carga archivo CSV desde Google Drive |
| `df.dropna()` | Elimina filas con valores faltantes |
| `df.apply()` | Aplica funci√≥n a cada fila para crear categor√≠as |

**Uso en este proyecto**: Manipulaci√≥n y limpieza de datos tabulares

---

### **NumPy**
```python
import numpy as np
```

- **Funci√≥n**: Operaciones num√©ricas subyacentes
- **Uso**: C√°lculos de m√©tricas (RMSE, MAE)
- **Por qu√©**: Base computacional eficiente para scikit-learn

---

### **Scikit-Learn**

#### `train_test_split`
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
```

- **Funci√≥n**: Divide datos en 70% entrenamiento y 30% prueba
- **random_state=42**: Garantiza reproducibilidad
- **Ventaja**: Eval√∫a el modelo en datos nunca vistos

---

#### `OneHotEncoder`
```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(handle_unknown="ignore")
```

- **Funci√≥n**: Convierte variable categ√≥rica en num√©ricas binarias
- **Ejemplo**:
  ```
  Entrada:  ["Normal", "Prediabetes", "Diabetes"]
  Salida:   [[1, 0, 0], [0, 1, 0], [0, 0, 1]]
  ```
- **handle_unknown="ignore"**: Maneja categor√≠as no vistas en entrenamiento

---

#### `ColumnTransformer`
```python
from sklearn.compose import ColumnTransformer

preprocessor = ColumnTransformer(
    transformers=[
        ("num", "passthrough", numeric_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols)
    ]
)
```

- **Funci√≥n**: Aplica transformaciones diferentes por tipo de columna
- **"passthrough"**: Deja columnas num√©ricas sin cambios
- **OneHotEncoder**: Transforma solo las categ√≥ricas
- **Ventaja**: Preprocesamiento heterog√©neo en una l√≠nea

---

#### `Pipeline`
```python
from sklearn.pipeline import Pipeline

model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", GradientBoostingRegressor(...))
])
```

- **Funci√≥n**: Encadena preprocesamiento y modelo
- **Ventaja**: Evita data leakage (fuga de informaci√≥n entre train/test)
- **Flujo**: `Datos Brutos ‚Üí Preprocessor ‚Üí Modelo ‚Üí Predicci√≥n`

---

#### `M√©tricas de Evaluaci√≥n`
```python
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

r2 = r2_score(y_real, y_predicho)
rmse = np.sqrt(mean_squared_error(y_real, y_predicho))
mae = mean_absolute_error(y_real, y_predicho)
```

| M√©trica | F√≥rmula | Interpretaci√≥n |
|---------|---------|----------------|
| **R¬≤** | `1 - (SS_res / SS_tot)` | Proporci√≥n de varianza explicada (0-1) |
| **RMSE** | `‚àö(Œ£(y_real - y_pred)¬≤ / n)` | Error promedio en mg/dL |
| **MAE** | `Œ£\|y_real - y_pred\| / n` | Error absoluto medio en mg/dL |

---

### **Joblib**
```python
import joblib

# Guardar modelo
joblib.dump(model, "modelo.joblib")

# Cargar modelo
modelo_cargado = joblib.load("modelo.joblib")
```

- **Funci√≥n**: Serializa modelos entrenados
- **Ventaja**: Reutilizar sin reentrenamiento

---

## üìä Caracter√≠sticas (Features)

El modelo utiliza **7 caracter√≠sticas** como entrada:

| Feature | Tipo | Descripci√≥n | Rango T√≠pico |
|---------|------|-------------|--------------|
| **Edad_A√±os** | Num√©rica | Edad del paciente | 18-100 a√±os |
| **peso** | Num√©rica | Peso corporal | kg |
| **talla** | Num√©rica | Altura/Talla | cm |
| **imc** | Num√©rica | √çndice de Masa Corporal | 10-50 kg/m¬≤ |
| **tas** | Num√©rica | Tensi√≥n Arterial Sist√≥lica | 80-200 mmHg |
| **tad** | Num√©rica | Tensi√≥n Arterial Diast√≥lica | 40-120 mmHg |
| **Categoria_Glucosa** | Categ√≥rica | Clasificaci√≥n previa de glucosa | Nominal |

### Variable Objetivo (Target)
- **"Resultado"**: Medici√≥n de glucosa en sangre (mg/dL)

---

## üè∑Ô∏è Clasificaci√≥n Cl√≠nica Autom√°tica

```python
def clasificar_glucosa(valor):
    if valor <= 100:
        return "Normal"
    elif valor <= 125:
        return "Prediabetes"
    else:
        return "Diabetes"
```

### Criterios de Clasificaci√≥n (OMS/ADA)

| Clasificaci√≥n | Rango (mg/dL) | Interpretaci√≥n |
|---------------|---------------|----------------|
| **Normal** | ‚â§ 100 | Glucosa normal en ayunas |
| **Prediabetes** | 101-125 | Riesgo de desarrollar diabetes |
| **Diabetes** | ‚â• 126 | Diabetes mellitus diagnosticada |

---

## üîÑ Estructura Com√∫n del C√≥digo

Todos los 7 modelos siguen esta estructura id√©ntica:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. CARGAR DATOS (CSV)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. CREAR CATEGOR√çAS DE GLUCOSA     ‚îÇ
‚îÇ     (Normal/Prediabetes/Diabetes)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. SELECCIONAR FEATURES (7)        ‚îÇ
‚îÇ     y TARGET (Resultado)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. LIMPIAR DATOS                   ‚îÇ
‚îÇ     dropna() en target              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. PREPROCESAR                     ‚îÇ
‚îÇ     Num√©ricas: passthrough          ‚îÇ
‚îÇ     Categ√≥ricas: OneHotEncoder      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. DIVIDIR DATOS                   ‚îÇ
‚îÇ     70% train / 30% test            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  7. CREAR PIPELINE                  ‚îÇ
‚îÇ     Preprocessor + Modelo           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  8. ENTRENAR MODELO                 ‚îÇ
‚îÇ     model.fit(X_train, y_train)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  9. EVALUAR                         ‚îÇ
‚îÇ     Calcular R¬≤, RMSE, MAE          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  10. GUARDAR MODELO                 ‚îÇ
‚îÇ      joblib.dump()                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìà Comparativa de Desempe√±o

Se entrenaron 7 modelos diferentes. Aqu√≠ est√° el ranking:

| Modelo | R¬≤ Test | RMSE Test | MAE Test |
|--------|---------|-----------|----------|
| **Random Forest** ü•á | 0.8622 | 12.62 mg/dL | 9.69 mg/dL |
| **Gradient Boosting** ü•à | 0.8240 | 14.27 mg/dL | 10.90 mg/dL |
| **Ridge Regression** ü•â | 0.8237 | 14.28 mg/dL | 11.26 mg/dL |
| Lasso Regression | 0.8237 | 14.28 mg/dL | 11.24 mg/dL |
| Regresi√≥n Lineal | 0.8233 | 14.29 mg/dL | 11.28 mg/dL |
| Support Vector Machine | 0.8114 | 14.77 mg/dL | 11.55 mg/dL |
| Red Neuronal (MLP) | 0.7956 | 15.37 mg/dL | 11.94 mg/dL |

---

## üéØ Modelos Implementados

### 1. Regresi√≥n Lineal
- **Caracter√≠sticas**: Simple, interpretable, baseline
- **Ventaja**: Muy r√°pida
- **Desventaja**: No captura no-linealidades
- **Caso de uso**: Comparaci√≥n base

### 2. Ridge Regression (L2 Regularization)
- **Caracter√≠sticas**: Regresi√≥n lineal con penalizaci√≥n
- **Ventaja**: Evita sobreajuste
- **Desventaja**: Mantiene todas las variables
- **Caso de uso**: Cuando se necesita estabilidad

### 3. Lasso Regression (L1 Regularization)
- **Caracter√≠sticas**: Regresi√≥n lineal con sparsity
- **Ventaja**: Selecciona autom√°ticamente features importantes
- **Desventaja**: Puede eliminar variables √∫tiles
- **Caso de uso**: Selecci√≥n de variables

### 4. Random Forest
- **Caracter√≠sticas**: Ensamble de √°rboles paralelos (RECOMENDADO)
- **Ventaja**: Mejor precisi√≥n (R¬≤=0.8622), robusto
- **Desventaja**: Requiere m√°s memoria
- **Caso de uso**: M√°xima precisi√≥n

### 5. Gradient Boosting
- **Caracter√≠sticas**: Ensamble secuencial de √°rboles d√©biles
- **Ventaja**: Muy buena precisi√≥n (R¬≤=0.8240)
- **Desventaja**: Lento en entrenamiento
- **Caso de uso**: Datos complejos con relaciones no-lineales

### 6. Support Vector Machine (SVM)
- **Caracter√≠sticas**: Busca hiperplano √≥ptimo
- **Ventaja**: Bueno en espacios de alta dimensi√≥n
- **Desventaja**: R¬≤ menor (0.8114)
- **Caso de uso**: Cuando hay muchas features

### 7. Red Neuronal (MLP)
- **Caracter√≠sticas**: Redes con capas densas
- **Ventaja**: Flexible, aprende patrones complejos
- **Desventaja**: Desempe√±o menor (R¬≤=0.7956), requiere m√°s datos
- **Caso de uso**: Datasets muy grandes

---

## üî® Ejemplo de C√≥digo: Gradient Boosting

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib

# 1. Cargar datos
df = pd.read_csv("Glucosa_Unique_Completo.csv")

# 2. Crear categor√≠as
def clasificar_glucosa(valor):
    if valor <= 100:
        return "Normal"
    elif valor <= 125:
        return "Prediabetes"
    else:
        return "Diabetes"

df["Categoria_Glucosa"] = df["Resultado"].apply(clasificar_glucosa)

# 3. Seleccionar features
features_seleccionadas = [
    "Edad_A√±os", "peso", "talla", "imc", "tas", "tad", "Categoria_Glucosa"
]
target = "Resultado"

df_limpio = df.dropna(subset=features_seleccionadas + [target]).copy()
X = df_limpio[features_seleccionadas]
y = df_limpio[target]

# 4. Preprocesador
numeric_cols = X.select_dtypes(include=["int64", "float64"]).columns
categorical_cols = X.select_dtypes(exclude=["int64", "float64"]).columns

preprocessor = ColumnTransformer(
    transformers=[
        ("num", "passthrough", numeric_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols)
    ]
)

# 5. Pipeline con Gradient Boosting
model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", GradientBoostingRegressor(
        n_estimators=200,
        learning_rate=0.1,
        max_depth=5,
        random_state=42
    ))
])

# 6. Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 7. Entrenar
model.fit(X_train, y_train)

# 8. Evaluar
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)

r2_train = r2_score(y_train, y_pred_train)
rmse_train = (mean_squared_error(y_train, y_pred_train))**0.5
mae_train = mean_absolute_error(y_train, y_pred_train)

r2_test = r2_score(y_test, y_pred_test)
rmse_test = (mean_squared_error(y_test, y_pred_test))**0.5
mae_test = mean_absolute_error(y_test, y_pred_test)

print("="*50)
print("EVALUACI√ìN DEL MODELO GRADIENT BOOSTING")
print("="*50)
print(f"[ENTRENAMIENTO] R¬≤={r2_train:.3f} | RMSE={rmse_train:.2f} mg/dL | MAE={mae_train:.2f} mg/dL")
print(f"[PRUEBA       ] R¬≤={r2_test:.3f} | RMSE={rmse_test:.2f} mg/dL | MAE={mae_test:.2f} mg/dL")
print("="*50)

# 9. Guardar
joblib.dump(model, "modelo_gradient_boosting.joblib")
print(f"‚úÖ Modelo guardado")
```

---

## üíæ C√≥mo Usar un Modelo Entrenado

```python
import joblib
import pandas as pd

# Cargar modelo
modelo = joblib.load("modelo_gradient_boosting.joblib")

# Preparar datos nuevos
X_nuevo = pd.DataFrame({
    "Edad_A√±os": [45],
    "peso": [75],
    "talla": [170],
    "imc": [26],
    "tas": [120],
    "tad": [80],
    "Categoria_Glucosa": ["Normal"]
})

# Predecir
prediccion = modelo.predict(X_nuevo)
print(f"Glucosa predicha: {prediccion[0]:.2f} mg/dL")
```

---

## üìä Interpretaci√≥n de M√©tricas

### R¬≤ (Coeficiente de Determinaci√≥n)
- **Rango**: 0 a 1 (m√°s alto es mejor)
- **Interpretaci√≥n**: Proporci√≥n de varianza explicada
- **Ejemplo**: R¬≤=0.86 = El modelo explica el 86% de la variabilidad

### RMSE (Root Mean Squared Error)
- **Unidad**: mg/dL
- **Interpretaci√≥n**: Error promedio esperado
- **Ventaja**: Penaliza errores grandes

### MAE (Mean Absolute Error)
- **Unidad**: mg/dL
- **Interpretaci√≥n**: Error absoluto promedio
- **Ventaja**: M√°s intuitivo que RMSE

---

## üìÅ Estructura de Archivos

```
proyecto-glucosa-ml/
‚îÇ
‚îú‚îÄ‚îÄ README.md                                    # Este archivo
‚îú‚îÄ‚îÄ Glucosa_Unique_Completo.csv                  # Dataset
‚îÇ
‚îú‚îÄ‚îÄ modelos/
‚îÇ   ‚îú‚îÄ‚îÄ train_linear_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ train_ridge_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ train_lasso_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ train_random_forest.py
‚îÇ   ‚îú‚îÄ‚îÄ train_gradient_boosting.py               # Ejemplo del c√≥digo
‚îÇ   ‚îú‚îÄ‚îÄ train_svm.py
‚îÇ   ‚îî‚îÄ‚îÄ train_mlp.py
‚îÇ
‚îî‚îÄ‚îÄ modelos_entrenados/
    ‚îú‚îÄ‚îÄ modelo_linear_regression.joblib
    ‚îú‚îÄ‚îÄ modelo_ridge_regression.joblib
    ‚îú‚îÄ‚îÄ modelo_lasso_regression.joblib
    ‚îú‚îÄ‚îÄ modelo_random_forest.joblib
    ‚îú‚îÄ‚îÄ modelo_gradient_boosting.joblib
    ‚îú‚îÄ‚îÄ modelo_svm.joblib
    ‚îî‚îÄ‚îÄ modelo_mlp.joblib
```

---

## ‚úÖ Ventajas de Esta Estructura

- ‚úÖ **Modular**: F√°cil agregar nuevos modelos
- ‚úÖ **Reproducible**: random_state garantiza resultados id√©nticos
- ‚úÖ **Escalable**: Preprocesamiento automatizado
- ‚úÖ **Reutilizable**: Pipeline encapsulado
- ‚úÖ **Comparable**: Todos los modelos con misma metodolog√≠a

---

## ‚ö†Ô∏è Limitaciones

- ‚ö†Ô∏è Requiere datos de calidad bien estructurados
- ‚ö†Ô∏è Error de ¬±12-15 mg/dL requiere confirmaci√≥n m√©dica
- ‚ö†Ô∏è No sustituye diagn√≥stico profesional
- ‚ö†Ô∏è Modelos espec√≠ficos para predicci√≥n de glucosa

---

## üöÄ Mejoras Futuras

1. Validaci√≥n cruzada K-Fold
2. Hiperpar√°metro tuning autom√°tico (Grid Search)
3. Feature importance analysis
4. Comparativa visual con gr√°ficos
5. API REST para despliegue
6. Dashboard interactivo (Streamlit)

---

## üìñ Referencias Cl√≠nicas

- OMS (2006): Definici√≥n y diagn√≥stico de diabetes mellitus
- ADA Standards of Care: Criterios de clasificaci√≥n
- Friedman (2001): Gradient Boosting Machines
- Chen & Guestrin (2016): XGBoost - Scalable Tree Boosting

---

## üìß Contacto

Para preguntas o sugerencias: [tu email]

---

**√öltima actualizaci√≥n**: Octubre 2025
**Versi√≥n**: 1.0
